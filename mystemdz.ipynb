{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Гриша Кузнецов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка mystem\n",
    "Тут я импортирую нужные модули\n",
    "1) Использую консольный mystem с параметрами -ln. Так я выведу только леммы, по одной на каждой строке\n",
    "2) Из них я делаю массив лемм\n",
    "3) Затем записываю этот массив в json с помощью dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Гриша\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.94 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import timeit\n",
    "import time\n",
    "import json\n",
    "from string import punctuation \n",
    "from collections import Counter\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "\n",
    "json_lemms = []\n",
    "%time os.system('.\\mystem.exe -inc --format json book.txt bookout.json')\n",
    "with open('bookout.json', 'r', encoding='utf-8') as f:\n",
    "    lemms = f.readlines()\n",
    "    for lemm in lemms:\n",
    "        json_lemms.append(json.loads(lemm))\n",
    "with open('lemms_out.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_lemms, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка pymorphy\n",
    "1) Открываю исходный текст книги, привожу в опрятный вид и токенизирую его. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 613 ms\n"
     ]
    }
   ],
   "source": [
    "with open('book.txt', 'r', encoding='utf-8') as f:\n",
    "    booktext = f.read()\n",
    "for i in punctuation:\n",
    "    booktext = booktext.replace(i, '')\n",
    "%time tokens = (word_tokenize(booktext))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение в json\n",
    "1) Ищу аннотацию для каждого токена\n",
    "2) Достаю оттуда часть, содержащее слово, массив тегов, вероятность\n",
    "3) Чтобы засунуть теги в JSON, я привел tag к формату str (с помощью str(), не знаю, насколько это правильно, но работает)\n",
    "4) Создаю словарь, где ключ - слово, значение - массив его характеристик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "anno_d = {}\n",
    "for token in tokens:\n",
    "    annotation = morph.parse(token)\n",
    "    parse = annotation[0]\n",
    "    tag = parse.tag\n",
    "    tagstr = str(tag)\n",
    "    tagstr = tagstr.split(',')\n",
    "    anno_d[parse.word] = tagstr, parse.normal_form, parse.score\n",
    "with open('tags.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(anno_d, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какую долю слов составляет каждая часть речи? (Например, для глагола - это количество глаголов, деленное на общее число слов в тексте)\n",
    "1) Тут я делаю массивы для частей речи, глаголов и наречий\n",
    "2) Прохожусь по всем токенам, паршу, беру содержательную часть Parse как в предыдущем задании\n",
    "3) Отбираю глаголы и наречия в свои массивы\n",
    "4) Делаю общий массив всех значений Part Of Speech\n",
    "5) Считаю токены для процентных соотношений "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_a = []\n",
    "token_count = 0\n",
    "verbs = []\n",
    "adverbs = []\n",
    "for token in tokens:\n",
    "    annotation = morph.parse(token)\n",
    "    parse = annotation[0]\n",
    "    if parse.tag.POS == 'VERB':\n",
    "        verbs.append(parse.word)\n",
    "    if parse.tag.POS == 'ADVB':\n",
    "        adverbs.append(parse.word)\n",
    "    if parse.tag.POS is not None:\n",
    "        pos_a.append(parse.tag.POS)\n",
    "        token_count += 1\n",
    "pos_c = Counter(pos_a)\n",
    "for key, value in pos_c.items():\n",
    "    percent = (value / token_count) * 100\n",
    "    print(key, round(percent, 2), '%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Найдите топ-20 (по частотности) глаголов и наречий\n",
    "### Глаголы\n",
    "Делаю Counter по списку глаголов и вывожу их вместе с количеством употреблений в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_c = Counter(verbs)\n",
    "adverbs_c = Counter(adverbs)\n",
    "print('TOP-20 verbs: ')\n",
    "for i in verbs_c.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наречия\n",
    "Делаю то же самое для наречий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TOP-20 adverbs: ')\n",
    "for i in adverbs_c.most_common(20):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Биграммы и триграммы\n",
    "### Биграммы\n",
    "1) Использую функцию для биграмм, сразу делая массив\n",
    "2) Из массива делаю Counter и вывожу биграмму с количеством взождений в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list(nltk.bigrams(tokens))\n",
    "bi_c = Counter(bigrams)\n",
    "for i in bi_c.most_common(25):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Триграммы\n",
    "Делаю то же самое для триграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = list(nltk.trigrams(tokens))\n",
    "tri_c = Counter(trigrams)\n",
    "for i in tri_c.most_common(25):\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
